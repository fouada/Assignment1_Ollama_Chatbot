# Ollama Chatbot - Private Local AI Assistant

[![Python](https://img.shields.io/badge/Python-3.13%2B-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.51.0-FF4B4B.svg)](https://streamlit.io/)
[![Flask](https://img.shields.io/badge/Flask-3.1.2-000000.svg)](https://flask.palletsprojects.com/)
[![Ollama](https://img.shields.io/badge/Ollama-0.12.9-orange.svg)](https://ollama.ai/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## üìã Table of Contents

1. [Abstract](#-abstract)
2. [What Does This Repository Do?](#-what-does-this-repository-do)
3. [Features](#-features)
4. [Software Requirements](#-software-requirements)
5. [Installation](#-installation)
6. [How to Operate](#-how-to-operate)
7. [Project Structure](#-project-structure)
8. [Testing](#-testing)
9. [Instructions & Roles](#-instructions--roles)
10. [Edge Cases & Error Handling](#-edge-cases--error-handling)
11. [Troubleshooting](#-troubleshooting)
12. [Documentation](#-documentation)
13. [Contributing](#-contributing)
14. [License](#-license)

---

## üìñ Abstract

**Ollama Chatbot** is a privacy-first, cost-free local AI chatbot system that provides a luxurious ChatGPT-like experience entirely on your machine. This repository contains a complete solution with both a web-based user interface (Streamlit) and a REST API (Flask) for interacting with locally-hosted Large Language Models (LLMs) via Ollama.

**Key Characteristics:**
- üîí **100% Private** - All data processing happens locally; no data leaves your machine
- üí∞ **Zero Cost** - No API fees, no subscriptions, completely free to use
- ‚ö° **Fast** - Direct local API calls with no network latency
- üé® **Luxurious UI** - Modern, gradient-based interface similar to ChatGPT, Claude, and Gemini
- ü§ñ **Multi-Model** - Support for multiple LLM models (llama3.2, mistral, phi, codellama)
- üõ°Ô∏è **No Internet Required** - Works completely offline after initial setup
- üîì **No API Keys** - No external authentication or registration needed

This project is ideal for:
- Privacy-conscious users who want AI capabilities without cloud dependency
- Students and researchers who need cost-free AI tools
- Developers building local AI applications
- Organizations with strict data privacy requirements

---

## üéØ What Does This Repository Do?

This repository provides a **complete local AI chatbot solution** with two interfaces:

### 1. **Streamlit Web Interface** (`apps/app_streamlit.py`)
A luxurious, ChatGPT-like web application that provides:
- Real-time streaming chat responses (word-by-word display)
- Model selection dropdown (choose between llama3.2, mistral, phi, codellama)
- Temperature control slider (0.0-2.0) for adjusting AI creativity
- Chat history management with session persistence
- Connection status monitoring
- Session statistics tracking
- Privacy feature indicators
- Clear conversation functionality
- Responsive, gradient-based UI design

**Use Case:** Interactive conversations with AI through a beautiful web interface

### 2. **Flask REST API** (`apps/app_flask.py`)
A comprehensive REST API for programmatic access with 5 endpoints:
- `GET /` - API information and documentation
- `GET /health` - Health check and Ollama connectivity status
- `GET /models` - List all available Ollama models
- `POST /chat` - Chat with AI (supports both streaming and non-streaming)
- `POST /generate` - Generate text completions

**Use Case:** Integration with other applications, automation, scripting, testing

### 3. **Automation Scripts** (`scripts/`)
Professional launcher scripts that handle:
- Environment validation
- Ollama connectivity checks
- Virtual environment activation
- Package verification
- Automated application launching
- Comprehensive testing suite

---

## ‚ú® Features

### Core Features
- ‚úÖ **Local Processing** - All AI inference happens on your machine
- ‚úÖ **Streaming Responses** - Real-time token-by-token response display
- ‚úÖ **Multi-Model Support** - Switch between different LLM models
- ‚úÖ **Temperature Control** - Adjust response creativity (deterministic ‚Üî creative)
- ‚úÖ **Chat History** - Session-based conversation tracking
- ‚úÖ **REST API** - Full programmatic access via HTTP endpoints
- ‚úÖ **Health Monitoring** - Server status and connectivity checks
- ‚úÖ **Error Handling** - Graceful error messages and recovery
- ‚úÖ **Logging** - Configurable logging levels (DEBUG, INFO, WARNING, ERROR)

### Technical Features
- ‚úÖ **Python 3.13** - Latest Python with performance improvements
- ‚úÖ **UV Package Manager** - Ultra-fast dependency installation (10-100x faster than pip)
- ‚úÖ **Virtual Environment** - Isolated Python environment
- ‚úÖ **Automated Testing** - Comprehensive validation suite
- ‚úÖ **Professional Structure** - Clean, organized codebase
- ‚úÖ **Comprehensive Documentation** - PRD, API docs, conversation logs

---

## üíª Software Requirements

### Required Software Packages

| Software | Version | Purpose | Installation |
|----------|---------|---------|--------------|
| **Python** | 3.13+ | Runtime environment | `brew install python3` |
| **Ollama** | 0.12.9+ | Local LLM server | `brew install ollama` |
| **UV** | 0.9.7+ | Fast package manager | `curl -LsSf https://astral.sh/uv/install.sh \| sh` |
| **Git** | 2.x+ | Version control | Pre-installed on macOS |

### Python Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| **streamlit** | ‚â•1.39.0 | Web UI framework |
| **flask** | ‚â•3.0.0 | REST API framework |
| **ollama** | ‚â•0.3.0 | Ollama Python client |
| **requests** | ‚â•2.32.0 | HTTP library |
| **python-dotenv** | ‚â•1.0.0 | Environment configuration |

All Python dependencies are automatically installed via `requirements.txt`.

### Required LLM Models

The following models are recommended (choose at least one):

| Model | Size | Parameters | Best For | Pull Command |
|-------|------|------------|----------|--------------|
| **llama3.2** | 2.0 GB | 3.2B | General purpose, balanced | `ollama pull llama3.2` |
| **mistral** | 4.1 GB | 7B | Powerful, fast responses | `ollama pull mistral` |
| **phi** | 1.6 GB | 2.7B | Compact, efficient | `ollama pull phi` |
| **codellama** | 3.8 GB | 7B | Code generation | `ollama pull codellama` |

**Total Recommended Size:** ~11.5 GB (all 4 models)

### System Requirements

- **Operating System:** macOS, Linux, or Windows
- **RAM:** Minimum 8GB (16GB+ recommended for larger models)
- **Storage:** 15GB free space (for models and dependencies)
- **CPU:** Modern multi-core processor (Apple Silicon or x86_64)
- **Internet:** Required only for initial setup (model downloads)

---

## üöÄ Installation

### Step 1: Clone the Repository

```bash
# Clone from GitHub
git clone https://github.com/fouada/Assignment1_Ollama_Chatbot.git

# Navigate to project directory
cd Assignment1_Ollama_Chatbot
```

### Step 2: Install System Dependencies

#### Install Python 3.13 (if not already installed)
```bash
# macOS
brew install python3

# Verify installation
python3 --version  # Should show Python 3.13.x
```

#### Install Ollama
```bash
# macOS
brew install ollama

# Verify installation
ollama --version  # Should show ollama version 0.12.9 or higher
```

#### Install UV Package Manager
```bash
# Install UV for fast package management
curl -LsSf https://astral.sh/uv/install.sh | sh

# Restart terminal or source profile
source ~/.bashrc  # or ~/.zshrc for zsh

# Verify installation
uv --version  # Should show uv 0.9.7 or higher
```

### Step 3: Start Ollama Server

```bash
# Start Ollama as a service (runs in background)
brew services start ollama

# OR start Ollama manually (runs in foreground)
ollama serve
```

**Verify Ollama is running:**
```bash
curl http://localhost:11434/api/tags
# Should return JSON with available models
```

### Step 4: Pull LLM Models

Download at least one model (or all recommended models):

```bash
# Essential: General purpose model (2.0 GB)
ollama pull llama3.2

# Optional: Powerful model (4.1 GB)
ollama pull mistral

# Optional: Compact model (1.6 GB)
ollama pull phi

# Optional: Code specialist (3.8 GB)
ollama pull codellama
```

**Note:** If behind a corporate proxy, configure proxy settings:
```bash
HTTP_PROXY=http://your-proxy:port \
HTTPS_PROXY=http://your-proxy:port \
ollama serve &
```

**Verify models are installed:**
```bash
ollama list
# Should show all downloaded models
```

### Step 5: Create Virtual Environment

```bash
# Create virtual environment using UV (fast)
uv venv

# Activate virtual environment
source .venv/bin/activate  # macOS/Linux
# OR
.venv\Scripts\activate  # Windows
```

**Your terminal prompt should now show `(.venv)` indicating the virtual environment is active.**

### Step 6: Install Python Dependencies

```bash
# Install all required packages using UV (fast)
uv pip install -r requirements.txt

# This installs:
# - streamlit (web UI)
# - flask (REST API)
# - ollama (Python client)
# - requests (HTTP library)
# - python-dotenv (environment config)
# + ~45 additional dependencies
```

**Expected output:**
```
Resolved 50 packages in 234ms
Downloaded 50 packages in 1.2s
Installed 50 packages in 396ms
 + streamlit==1.51.0
 + flask==3.1.2
 + ollama==0.6.0
 + ... (47 more packages)
```

### Step 7: Verify Installation

Run the comprehensive test suite:

```bash
# Navigate to scripts directory
cd scripts

# Run test suite
./run_tests.sh
```

**Expected output:**
```
========================================
  Ollama Chatbot Test Suite
========================================

Test 1: Ollama Server Connection
‚úì PASSED - Ollama server is reachable

Test 2: Virtual Environment
‚úì PASSED - Virtual environment exists

Test 3: Package Imports
  ‚úì streamlit
  ‚úì flask
  ‚úì requests
  ‚úì ollama

Test 4: Ollama Models Availability
‚úì PASSED - 4 model(s) available

Test 5: Ollama API Response
‚úì PASSED - API response successful

========================================
  Test Summary
========================================
  Passed: 9
  Failed: 0

‚úÖ All tests passed! System ready.
```

**If all tests pass, installation is complete!** ‚úÖ

---

## üéÆ How to Operate

### Option 1: Streamlit Web Interface (Recommended for Interactive Use)

#### Launching the Application

```bash
# From project root directory
cd scripts

# Launch Streamlit chatbot
./launch_streamlit.sh
```

**What happens:**
1. Script checks Ollama server connectivity
2. Activates virtual environment
3. Verifies all required packages are installed
4. Sets environment variables (port, logging level)
5. Launches Streamlit application
6. Opens browser automatically at `http://localhost:8501`

**Expected output:**
```
========================================
  Ollama Chatbot (Streamlit) Launcher
========================================

[1/5] Checking Ollama server...
‚úì Ollama server is running

[2/5] Activating virtual environment...
‚úì Virtual environment activated

[3/5] Verifying packages...
  ‚úì streamlit
  ‚úì requests
  ‚úì ollama
‚úì All packages verified

[4/5] Setting environment variables...
  ‚úì Port: 8501
  ‚úì Log Level: INFO
  ‚úì Ollama API: http://localhost:11434

[5/5] Launching Streamlit application...

========================================
  Ollama Chatbot (Streamlit) is starting...
========================================

  üåê URL: http://localhost:8501
  üìù Log Level: INFO
  ü§ñ Ollama: Connected

Press Ctrl+C to stop the server

You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://192.168.1.x:8501
```

#### Using the Web Interface

1. **Browser Opens Automatically**
   - URL: `http://localhost:8501`
   - Modern, gradient-based UI appears

2. **Select Model** (Sidebar)
   - Choose from available models:
     - ü¶ô llama3.2 - General purpose
     - ‚ö° mistral - Powerful & fast
     - üß† phi - Compact & efficient
     - üíª codellama - Code specialist

3. **Adjust Temperature** (Sidebar)
   - Slider: 0.0 - 2.0
   - üéØ Focused (< 0.5) - Deterministic responses
   - ‚öñÔ∏è Balanced (0.5-1.0) - Natural conversation
   - üé® Creative (> 1.0) - More random, creative

4. **Start Chatting**
   - Type message in chat input box at bottom
   - Press Enter or click send
   - Watch AI response stream word-by-word
   - Continue conversation naturally

5. **Manage Conversation**
   - View session statistics in sidebar
   - Clear chat history with button
   - All data stays in browser session

6. **Stop the Application**
   - Press `Ctrl+C` in terminal
   - Application stops gracefully

---

### Option 2: Flask REST API (Recommended for Programmatic Use)

#### Launching the API

```bash
# From project root directory
cd scripts

# Launch Flask API
./launch_flask.sh
```

**Expected output:**
```
========================================
  Ollama API (Flask) Launcher
========================================

[1/5] Checking Ollama server...
‚úì Ollama server is running

[2/5] Activating virtual environment...
‚úì Virtual environment activated

[3/5] Verifying packages...
  ‚úì flask
  ‚úì requests
  ‚úì ollama
‚úì All packages verified

[4/5] Setting environment variables...
  ‚úì Port: 5000
  ‚úì Log Level: INFO
  ‚úì Ollama API: http://localhost:11434

[5/5] Launching Flask application...

========================================
  Ollama API (Flask) is starting...
========================================

  üåê API URL: http://localhost:5000
  üìù Log Level: INFO
  ü§ñ Ollama: Connected
  üìä Health: http://localhost:5000/health

Press Ctrl+C to stop the server

 * Serving Flask app 'app_flask'
 * Debug mode: on
 * Running on http://0.0.0.0:5000
```

#### Using the REST API

**1. Check API Information**
```bash
curl http://localhost:5000/
```

**Response:**
```json
{
  "name": "Ollama Flask REST API",
  "version": "1.0.0",
  "description": "Private, local AI chatbot API",
  "features": [
    "100% Private - No data leaves your machine",
    "Cost-Free - No API fees",
    "Fast - Direct local processing",
    "Multiple Models - Choose your AI"
  ],
  "endpoints": {
    "GET /": "API information",
    "GET /health": "Health check",
    "GET /models": "List available models",
    "POST /chat": "Chat with AI (streaming)",
    "POST /generate": "Generate response (non-streaming)"
  },
  "timestamp": "2025-11-06T10:30:00.123456"
}
```

**2. Health Check**
```bash
curl http://localhost:5000/health
```

**Response:**
```json
{
  "status": "healthy",
  "ollama": "connected",
  "models_available": 4,
  "timestamp": "2025-11-06T10:30:00.123456"
}
```

**3. List Available Models**
```bash
curl http://localhost:5000/models
```

**Response:**
```json
{
  "models": [
    {
      "name": "llama3.2:latest",
      "size": 2000000000,
      "modified_at": "2025-11-06T09:00:00",
      "details": {
        "format": "gguf",
        "family": "llama",
        "parameter_size": "3.2B"
      }
    }
  ],
  "count": 4,
  "timestamp": "2025-11-06T10:30:00.123456"
}
```

**4. Chat with AI (Streaming)**
```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Explain quantum computing in simple terms",
    "model": "llama3.2",
    "temperature": 0.7,
    "stream": true
  }'
```

**Response (Server-Sent Events):**
```
data: {"content": "Quantum"}
data: {"content": " computing"}
data: {"content": " is"}
data: {"content": " a"}
data: {"content": " revolutionary"}
...
data: {"done": true}
```

**5. Chat with AI (Non-Streaming)**
```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is Python?",
    "model": "llama3.2",
    "temperature": 0.7,
    "stream": false
  }'
```

**Response:**
```json
{
  "response": "Python is a high-level, interpreted programming language known for its simplicity and readability...",
  "model": "llama3.2",
  "timestamp": "2025-11-06T10:30:00.123456"
}
```

**6. Generate Text Completion**
```bash
curl -X POST http://localhost:5000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "def factorial(n):",
    "model": "codellama",
    "temperature": 0.3
  }'
```

**Response:**
```json
{
  "response": "\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)",
  "model": "codellama",
  "timestamp": "2025-11-06T10:30:00.123456"
}
```

---

### Option 3: Direct Python Usage

You can also use the Ollama Python client directly:

```python
import ollama

# Simple chat
response = ollama.chat(
    model='llama3.2',
    messages=[{'role': 'user', 'content': 'Hello!'}]
)
print(response['message']['content'])

# Streaming chat
response = ollama.chat(
    model='llama3.2',
    messages=[{'role': 'user', 'content': 'Tell me a story'}],
    stream=True
)

for chunk in response:
    print(chunk['message']['content'], end='', flush=True)
```

---

## üìÅ Project Structure

```
Assignment1_Ollama_Chatbot/
‚îÇ
‚îú‚îÄ‚îÄ apps/                          # Application files
‚îÇ   ‚îú‚îÄ‚îÄ app_streamlit.py          # Streamlit web interface (385 lines)
‚îÇ   ‚îî‚îÄ‚îÄ app_flask.py              # Flask REST API (272 lines)
‚îÇ
‚îú‚îÄ‚îÄ scripts/                       # Automation scripts
‚îÇ   ‚îú‚îÄ‚îÄ launch_streamlit.sh       # Streamlit launcher with validation
‚îÇ   ‚îú‚îÄ‚îÄ launch_flask.sh           # Flask launcher with validation
‚îÇ   ‚îî‚îÄ‚îÄ run_tests.sh              # Comprehensive test suite
‚îÇ
‚îú‚îÄ‚îÄ tests/                         # Unit tests (future)
‚îÇ   ‚îî‚îÄ‚îÄ (test files to be added)
‚îÇ
‚îú‚îÄ‚îÄ docs/                          # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ PRD.md                    # Product Requirements Document
‚îÇ   ‚îî‚îÄ‚îÄ PROMPTS.md                # Complete development log
‚îÇ
‚îú‚îÄ‚îÄ .venv/                         # Virtual environment (auto-generated)
‚îÇ   ‚îî‚îÄ‚îÄ (Python packages)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ .gitignore                    # Git ignore rules
‚îú‚îÄ‚îÄ README.md                     # This file
‚îî‚îÄ‚îÄ LICENSE                       # MIT License
```

### File Descriptions

| File | Lines | Purpose | Role |
|------|-------|---------|------|
| **app_streamlit.py** | 385 | Web UI application | User interface for interactive chat |
| **app_flask.py** | 272 | REST API server | Programmatic access to AI capabilities |
| **launch_streamlit.sh** | 121 | Streamlit launcher | Automated setup and validation for Streamlit |
| **launch_flask.sh** | 120 | Flask launcher | Automated setup and validation for Flask |
| **run_tests.sh** | 117 | Test suite | Comprehensive validation of system |
| **requirements.txt** | 13 | Dependency list | Python package specifications |
| **PRD.md** | 903 | Product requirements | Complete product specification |
| **PROMPTS.md** | 3050 | Development log | Full conversation history |
| **README.md** | 1400+ | Main documentation | This comprehensive guide |

---

## üß™ Testing

### Test Suite Overview

The project includes a comprehensive test suite that validates all critical components before launching the application.

**Test File:** `scripts/run_tests.sh`

### What the Tests Do

The test suite performs **5 critical validations**:

#### Test 1: Ollama Server Connection
**Purpose:** Verify that Ollama server is running and reachable

**What it does:**
- Makes HTTP request to `http://localhost:11434/api/tags`
- Checks for successful response
- Validates server is accepting connections

**Expected Result:**
```
‚úì PASSED - Ollama server is reachable
```

**Success Criteria:**
- HTTP response received within 5 seconds
- Status code 200 or valid JSON response
- Server is accessible on port 11434

**Failure Indicators:**
```
‚úó FAILED - Ollama server is NOT reachable
```

**Why it fails:**
- Ollama server not started
- Port 11434 blocked or in use
- Firewall blocking connection
- Ollama crashed or misconfigured

**How to fix:**
```bash
# Start Ollama service
brew services start ollama

# OR run manually
ollama serve
```

---

#### Test 2: Virtual Environment
**Purpose:** Verify that Python virtual environment exists

**What it does:**
- Checks for `.venv` directory
- Validates directory structure
- Ensures virtual environment is properly created

**Expected Result:**
```
‚úì PASSED - Virtual environment exists
```

**Success Criteria:**
- `.venv/` directory exists in project root
- Directory contains `bin/`, `lib/`, and `pyvenv.cfg`
- Virtual environment is valid and activatable

**Failure Indicators:**
```
‚úó FAILED - Virtual environment not found
```

**Why it fails:**
- Virtual environment not created
- Directory deleted or moved
- Incorrect working directory

**How to fix:**
```bash
# Create virtual environment
uv venv

# Or use standard Python
python3 -m venv .venv
```

---

#### Test 3: Package Imports
**Purpose:** Verify all required Python packages are installed and importable

**What it does:**
- Activates virtual environment
- Attempts to import each required package:
  - `streamlit` (web UI framework)
  - `flask` (REST API framework)
  - `requests` (HTTP library)
  - `ollama` (Ollama Python client)
- Validates each import succeeds

**Expected Result:**
```
‚úì streamlit
‚úì flask
‚úì requests
‚úì ollama
```

**Success Criteria:**
- All 4 packages import without errors
- Packages are correct versions
- No missing dependencies

**Failure Indicators:**
```
‚úó streamlit (or any package name)
```

**Why it fails:**
- Package not installed
- Wrong Python version
- Corrupted package installation
- Virtual environment not activated

**How to fix:**
```bash
# Reinstall all packages
source .venv/bin/activate
uv pip install -r requirements.txt

# Or install specific package
uv pip install streamlit
```

---

#### Test 4: Ollama Models Availability
**Purpose:** Verify at least one LLM model is installed and available

**What it does:**
- Uses Ollama Python client to list models
- Counts number of installed models
- Validates at least 1 model is available

**Expected Result:**
```
‚úì PASSED - 4 model(s) available
```

**Success Criteria:**
- At least 1 model installed
- `ollama.list()` returns valid response
- Models are properly downloaded and accessible

**Failure Indicators:**
```
‚úó FAILED - No models available
```

**Why it fails:**
- No models pulled/downloaded
- Models corrupted or deleted
- Ollama server can't access model directory
- Insufficient disk space

**How to fix:**
```bash
# Pull at least one model
ollama pull llama3.2

# Verify models are listed
ollama list
```

---

#### Test 5: Ollama API Response
**Purpose:** Verify Ollama can generate responses (end-to-end functional test)

**What it does:**
- Sends a test prompt to Ollama ("Hi")
- Limits response to 5 tokens (fast test)
- Validates response is generated successfully
- Tests complete inference pipeline

**Expected Result:**
```
‚úì PASSED - API response successful
```

**Success Criteria:**
- `ollama.generate()` completes without error
- Response contains generated text
- Model successfully processes prompt
- Full pipeline works (Python ‚Üí Ollama ‚Üí Model ‚Üí Response)

**Failure Indicators:**
```
‚úó FAILED - API response failed
```

**Why it fails:**
- Model not loaded in memory
- Insufficient RAM for model
- Ollama server error
- Model file corrupted
- CUDA/GPU issues (if using GPU)

**How to fix:**
```bash
# Test model manually
ollama run llama3.2
> Hi
# Should get a response

# Check Ollama logs
brew services restart ollama
tail -f ~/Library/Logs/Homebrew/ollama/stdout

# Try smaller model if memory issue
ollama pull phi  # Only 1.6GB
```

---

### Running the Tests

**Execute full test suite:**
```bash
cd scripts
./run_tests.sh
```

**Test Output Format:**
```
========================================
  Ollama Chatbot Test Suite
========================================

Test 1: Ollama Server Connection
‚úì PASSED - Ollama server is reachable

Test 2: Virtual Environment
‚úì PASSED - Virtual environment exists

Test 3: Package Imports
  ‚úì streamlit
  ‚úì flask
  ‚úì requests
  ‚úì ollama

Test 4: Ollama Models Availability
‚úì PASSED - 4 model(s) available

Test 5: Ollama API Response
‚úì PASSED - API response successful

========================================
  Test Summary
========================================
  Passed: 9
  Failed: 0

‚úÖ All tests passed! System ready.
```

### Success vs. Failure

**‚úÖ All Tests Passed:**
- Exit code: 0
- Message: "‚úÖ All tests passed! System ready."
- **Action:** Proceed to launch application
- System is fully operational

**‚ùå Some Tests Failed:**
- Exit code: 1
- Message: "‚ùå Some tests failed. Please fix issues."
- **Action:** Review failed tests and follow fix instructions
- Do NOT launch application until all tests pass

### Test Execution Time

- **Full suite:** ~5-10 seconds
- **Test 1-2:** < 1 second
- **Test 3:** ~1 second
- **Test 4:** ~1 second
- **Test 5:** ~3-5 seconds (model inference)

---

## üìö Instructions & Roles

### User Roles

#### 1. **End User** (Interactive Chatbot User)
**Responsibility:** Use the Streamlit web interface for AI conversations

**How to operate:**
```bash
cd scripts
./launch_streamlit.sh
# Open browser at http://localhost:8501
# Start chatting with AI
```

**What you can do:**
- Ask questions to AI
- Get instant responses
- Choose different AI models
- Adjust creativity level
- View chat history
- Clear conversations

**What you cannot do:**
- Modify API settings
- Access model files directly
- Change server configuration

---

#### 2. **Developer** (API Integrator)
**Responsibility:** Integrate Ollama chatbot into applications via REST API

**How to operate:**
```bash
cd scripts
./launch_flask.sh
# API available at http://localhost:5000
```

**What you can do:**
- Make HTTP requests to API endpoints
- Stream or non-stream responses
- Choose models programmatically
- Integrate into scripts/applications
- Automate AI interactions

**Example integration:**
```python
import requests

response = requests.post('http://localhost:5000/chat', json={
    'message': 'Hello AI',
    'model': 'llama3.2',
    'stream': False
})

print(response.json()['response'])
```

---

#### 3. **Administrator** (System Manager)
**Responsibility:** Maintain and configure the system

**How to operate:**
```bash
# Check system health
./scripts/run_tests.sh

# Monitor Ollama status
brew services info ollama

# View logs
tail -f ~/.local/share/ollama/logs/server.log

# Update models
ollama pull llama3.2

# Restart services
brew services restart ollama
```

**What you can do:**
- Install/remove models
- Configure ports and settings
- Monitor system performance
- Troubleshoot issues
- Update dependencies

---

#### 4. **Tester** (Quality Assurance)
**Responsibility:** Validate system functionality and report issues

**How to test:**
```bash
# Run full test suite
cd scripts
./run_tests.sh

# Test Streamlit UI manually
./launch_streamlit.sh
# Verify:
# - UI loads correctly
# - Can select models
# - Chat responses work
# - Streaming works
# - Clear chat works

# Test Flask API
./launch_flask.sh
# Test each endpoint:
curl http://localhost:5000/
curl http://localhost:5000/health
curl http://localhost:5000/models
curl -X POST http://localhost:5000/chat -H "Content-Type: application/json" -d '{"message":"test"}'

# Load testing (optional)
# Use tools like Apache Bench or Locust
ab -n 100 -c 10 http://localhost:5000/health
```

**What to verify:**
- All tests pass
- UI is responsive
- API returns correct responses
- Error messages are clear
- Performance is acceptable

---

### Configuration Roles

#### Environment Variables

**Streamlit Configuration:**
```bash
export STREAMLIT_SERVER_PORT=8501          # Web UI port
export STREAMLIT_LOGGER_LEVEL=INFO         # Logging level
export OLLAMA_API_URL=http://localhost:11434  # Ollama server
```

**Flask Configuration:**
```bash
export FLASK_APP=apps/app_flask.py         # Flask app file
export FLASK_ENV=development               # Development mode
export FLASK_RUN_PORT=5000                # API port
export FLASK_LOG_LEVEL=INFO               # Logging level
export OLLAMA_API_URL=http://localhost:11434  # Ollama server
```

#### Logging Levels

| Level | Usage | When to Use |
|-------|-------|-------------|
| **DEBUG** | Detailed diagnostic info | Development, troubleshooting |
| **INFO** | General informational messages | Normal operation (default) |
| **WARNING** | Warning messages | Potential issues |
| **ERROR** | Error messages | Errors that don't stop app |
| **CRITICAL** | Critical errors | Severe errors |

**Change logging level:**
```bash
# Edit launcher scripts
nano scripts/launch_streamlit.sh
# Change: LOG_LEVEL="DEBUG"

# Or set environment variable
export STREAMLIT_LOGGER_LEVEL=DEBUG
./scripts/launch_streamlit.sh
```

---

## ‚ö†Ô∏è Edge Cases & Error Handling

### Edge Case 1: Ollama Server Not Running

**Scenario:** User tries to launch application but Ollama is not running

**What happens:**
```
[1/5] Checking Ollama server...
‚úó Ollama server is NOT running!

Please start Ollama first:
  brew services start ollama
  OR
  ollama serve
```

**Error handling:**
- Launcher script detects Ollama is unreachable
- Displays clear error message
- Provides fix commands
- Exits gracefully (exit code 1)
- Prevents application from starting in broken state

**User action:**
```bash
# Start Ollama
brew services start ollama

# Retry launching
./launch_streamlit.sh
```

---

### Edge Case 2: No Models Installed

**Scenario:** User starts application but no LLM models are downloaded

**What happens:**

**In Streamlit UI:**
```
‚ö†Ô∏è No models found!

ollama pull llama3.2
```
- Application detects empty model list
- Displays warning message with fix command
- Stops rendering rest of UI
- Prevents errors from missing models

**In Tests:**
```
Test 4: Ollama Models Availability
‚úó FAILED - No models available
```

**User action:**
```bash
# Pull at least one model
ollama pull llama3.2

# Verify installation
ollama list

# Relaunch application
./launch_streamlit.sh
```

---

### Edge Case 3: Virtual Environment Missing

**Scenario:** User deleted `.venv/` folder or hasn't created it

**What happens:**
```
[2/5] Activating virtual environment...
‚úó Virtual environment not found!
Creating virtual environment...

[Auto-creates venv]
‚úì Virtual environment activated
```

**Error handling:**
- Launcher detects missing `.venv/` directory
- Automatically creates virtual environment
- Continues with package verification
- Self-healing behavior

**User action:** None required (automatic)

---

### Edge Case 4: Missing Python Packages

**Scenario:** Some packages are not installed or corrupted

**What happens:**
```
[3/5] Verifying packages...
  ‚úì streamlit
  ‚úó flask (missing)
  ‚úì requests
  ‚úì ollama

Missing packages detected!
Installing missing packages...

[Auto-installs missing packages]
‚úì All packages verified
```

**Error handling:**
- Launcher checks each required package
- Identifies missing packages
- Automatically installs missing packages
- Verifies installation succeeded

**User action:** None required (automatic)

---

### Edge Case 5: Invalid Model Name in API Request

**Scenario:** User requests non-existent model via API

**Request:**
```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello", "model": "gpt-4"}'
```

**Response:**
```json
{
  "error": "model 'gpt-4' not found, try pulling it first",
  "timestamp": "2025-11-06T10:30:00.123456"
}
```

**Error handling:**
- Flask API catches Ollama exception
- Returns 500 status code with error details
- Logs error message
- Provides helpful error message to user

**User action:**
```bash
# Check available models
curl http://localhost:5000/models

# Use valid model name
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello", "model": "llama3.2"}'
```

---

### Edge Case 6: Missing Required Field in API Request

**Scenario:** API request missing required "message" field

**Request:**
```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"model": "llama3.2"}'
```

**Response:**
```json
{
  "error": "Missing \"message\" in request",
  "timestamp": "2025-11-06T10:30:00.123456"
}
```
**Status Code:** 400 Bad Request

**Error handling:**
- Flask validates request body
- Checks for required fields
- Returns 400 status (client error)
- Provides specific error message

**User action:**
```bash
# Include required "message" field
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello", "model": "llama3.2"}'
```

---

### Edge Case 7: Port Already in Use

**Scenario:** Port 5000 or 8501 is already occupied by another application

**What happens:**
```
OSError: [Errno 48] Address already in use

Error: Port 5000 is already in use.
```

**Error handling (manual):**
```bash
# Find process using the port
lsof -i :5000

# Kill the process
kill -9 <PID>

# OR change port in launcher script
nano scripts/launch_flask.sh
# Change: PORT=5001

# Relaunch
./launch_flask.sh
```

---

### Edge Case 8: Out of Memory During Model Inference

**Scenario:** System runs out of RAM while running large model

**What happens:**
```
Error: Failed to generate response
Ollama error: out of memory
```

**Error handling:**
- Application catches exception
- Displays error to user
- Logs error details
- Application continues running (doesn't crash)

**User action:**
```bash
# Use smaller model
# In Streamlit: Select "phi" (1.6GB) instead of "mistral" (4.1GB)

# In API request:
curl -X POST http://localhost:5000/chat \
  -d '{"message": "Hello", "model": "phi"}'

# OR close other applications to free RAM
```

---

### Edge Case 9: Empty Message Input

**Scenario:** User submits empty or whitespace-only message

**Streamlit UI:**
- Chat input validates message is not empty
- Submit button disabled if input is empty
- No API call made

**Flask API:**
```bash
curl -X POST http://localhost:5000/chat \
  -d '{"message": "   "}'
```

**Response:**
```json
{
  "error": "Missing \"message\" in request",
  "timestamp": "2025-11-06T10:30:00.123456"
}
```

---

### Edge Case 10: Network Proxy Issues (Corporate Environment)

**Scenario:** Corporate proxy blocks Ollama model downloads

**What happens:**
```
Error: dial tcp: lookup registry.ollama.ai: no such host
```

**Error handling:**
```bash
# Start Ollama with proxy configuration
HTTP_PROXY=http://proxy.company.com:8080 \
HTTPS_PROXY=http://proxy.company.com:8080 \
ollama serve &

# Then pull models
ollama pull llama3.2
```

**User action:** Configure proxy settings before starting Ollama

---

### Error Logging

All errors are logged with timestamps:

**Streamlit:**
```
2025-11-06 10:30:00.123 - ERROR - Error generating response: model not found
```

**Flask:**
```
2025-11-06 10:30:00 - app_flask - ERROR - Error in chat: model 'invalid' not found
```

**Logs location:**
- Streamlit: Console output
- Flask: Console output + `~/.local/share/ollama/logs/`
- Ollama: `~/Library/Logs/Homebrew/ollama/` (macOS)

---

## üîß Troubleshooting

### Problem 1: Tests Fail with "Ollama server is NOT reachable"

**Symptoms:**
```
‚úó FAILED - Ollama server is NOT reachable
```

**Diagnosis:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Check Ollama service status
brew services info ollama
```

**Solutions:**
```bash
# Solution 1: Start Ollama service
brew services start ollama

# Solution 2: Run Ollama manually
ollama serve

# Solution 3: Restart Ollama
brew services restart ollama

# Solution 4: Check port is not blocked
lsof -i :11434
```

**Verification:**
```bash
# Should see Ollama process
ps aux | grep ollama

# Should get JSON response
curl http://localhost:11434/api/tags
```

---

### Problem 2: Application Shows "No models found"

**Symptoms:**
- Streamlit shows "‚ö†Ô∏è No models found!"
- Test 4 fails: "‚úó FAILED - No models available"

**Diagnosis:**
```bash
# List installed models
ollama list
# If empty, no models are installed
```

**Solutions:**
```bash
# Pull recommended model (2.0 GB)
ollama pull llama3.2

# Wait for download to complete
# Verify installation
ollama list

# Should show:
# NAME              ID            SIZE      MODIFIED
# llama3.2:latest   a80c4f17acd5  2.0 GB    Just now
```

**Verification:**
```bash
# Test model works
ollama run llama3.2
> Hello
# Should get a response
```

---

### Problem 3: "Package not found" errors during installation

**Symptoms:**
```
ERROR: Could not find a version that satisfies the requirement streamlit
```

**Diagnosis:**
```bash
# Check Python version
python3 --version
# Must be 3.13+

# Check pip/uv is working
uv --version
```

**Solutions:**
```bash
# Solution 1: Update UV
curl -LsSf https://astral.sh/uv/install.sh | sh

# Solution 2: Clear cache and reinstall
rm -rf .venv
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt

# Solution 3: Use pip instead
pip install -r requirements.txt

# Solution 4: Install packages individually
uv pip install streamlit
uv pip install flask
uv pip install ollama
uv pip install requests
```

---

### Problem 4: Streamlit shows blank page or won't load

**Symptoms:**
- Browser opens but shows blank page
- Loading indicator spins forever
- Console shows errors

**Diagnosis:**
```bash
# Check Streamlit is running
ps aux | grep streamlit

# Check port 8501 is accessible
curl http://localhost:8501
```

**Solutions:**
```bash
# Solution 1: Clear browser cache
# Chrome: Cmd+Shift+Delete > Clear cache

# Solution 2: Try different browser

# Solution 3: Clear Streamlit cache
rm -rf ~/.streamlit/cache/

# Solution 4: Restart application
# Press Ctrl+C
./scripts/launch_streamlit.sh

# Solution 5: Check for port conflicts
lsof -i :8501
# Kill conflicting process
kill -9 <PID>
```

---

### Problem 5: API returns 500 errors

**Symptoms:**
```json
{
  "error": "Internal server error",
  "timestamp": "2025-11-06T10:30:00"
}
```

**Diagnosis:**
```bash
# Check Flask logs in terminal
# Look for Python traceback

# Test Ollama directly
curl http://localhost:11434/api/tags

# Check model exists
ollama list | grep llama3.2
```

**Solutions:**
```bash
# Solution 1: Restart Flask API
# Press Ctrl+C
./scripts/launch_flask.sh

# Solution 2: Verify request format
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"test","model":"llama3.2"}'

# Solution 3: Check Ollama is running
brew services restart ollama

# Solution 4: Try different model
curl -X POST http://localhost:5000/chat \
  -d '{"message":"test","model":"phi"}'
```

---

### Problem 6: Slow responses or timeouts

**Symptoms:**
- Responses take > 30 seconds
- Request times out
- "Generating..." indicator spins forever

**Diagnosis:**
```bash
# Check available RAM
top -l 1 | grep PhysMem

# Check model size vs available RAM
ollama list

# Check CPU usage
top -l 1 | grep CPU
```

**Solutions:**
```bash
# Solution 1: Use smaller model
# Switch from mistral (4.1GB) to phi (1.6GB)

# Solution 2: Close other applications
# Free up RAM and CPU

# Solution 3: Restart Ollama
brew services restart ollama

# Solution 4: Reduce temperature (faster generation)
# In API: "temperature": 0.3
# In UI: Move slider to left

# Solution 5: Limit response length (API only)
curl -X POST http://localhost:5000/generate \
  -d '{"prompt":"Hi","model":"phi","max_tokens":50}'
```

---

### Problem 7: Corporate proxy blocks model downloads

**Symptoms:**
```
Error: dial tcp: lookup registry.ollama.ai: no such host
```

**Diagnosis:**
```bash
# Check proxy environment variables
echo $HTTP_PROXY
echo $HTTPS_PROXY

# Test direct connection
curl -I https://registry.ollama.ai
```

**Solutions:**
```bash
# Solution 1: Configure proxy for Ollama
HTTP_PROXY=http://proxy.company.com:8080 \
HTTPS_PROXY=http://proxy.company.com:8080 \
ollama serve &

# Solution 2: Set permanent proxy
# Add to ~/.zshrc or ~/.bashrc:
export HTTP_PROXY=http://proxy.company.com:8080
export HTTPS_PROXY=http://proxy.company.com:8080

# Solution 3: Download models outside corporate network
# At home:
ollama pull llama3.2

# Models sync automatically
```

---

### Problem 8: Permission denied errors

**Symptoms:**
```
Permission denied: './scripts/launch_streamlit.sh'
```

**Diagnosis:**
```bash
# Check file permissions
ls -la scripts/

# Should show: -rwxr-xr-x (executable)
```

**Solutions:**
```bash
# Make scripts executable
chmod +x scripts/launch_streamlit.sh
chmod +x scripts/launch_flask.sh
chmod +x scripts/run_tests.sh

# Or run all at once
chmod +x scripts/*.sh

# Verify permissions
ls -la scripts/
```

---

### Getting Help

If problems persist:

1. **Check logs:**
   ```bash
   # Ollama logs
   tail -f ~/Library/Logs/Homebrew/ollama/stdout

   # Application logs (in terminal)
   ```

2. **Check GitHub Issues:**
   - https://github.com/fouada/Assignment1_Ollama_Chatbot/issues

3. **Ollama Documentation:**
   - https://github.com/ollama/ollama

4. **Run diagnostics:**
   ```bash
   # System info
   python3 --version
   ollama --version
   uv --version

   # Test everything
   cd scripts
   ./run_tests.sh
   ```

---

## üìñ Documentation

Comprehensive documentation is available in the `docs/` directory:

| Document | Description | Link |
|----------|-------------|------|
| **PRD.md** | Complete product requirements, goals, user stories, specifications | [docs/PRD.md](docs/PRD.md) |
| **PROMPTS.md** | Full development conversation log with all prompts and responses | [docs/PROMPTS.md](docs/PROMPTS.md) |
| **README.md** | This file - main project documentation | [README.md](README.md) |

**Quick Links:**
- üìö **API Reference:** See [PRD.md - Section 5.2](docs/PRD.md#52-flask-rest-api) for API documentation
- üìù **Development Log:** See [PROMPTS.md](docs/PROMPTS.md) for complete conversation history

---

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:

### How to Contribute

1. **Fork the repository**
   ```bash
   # Click "Fork" on GitHub
   ```

2. **Clone your fork**
   ```bash
   git clone https://github.com/YOUR_USERNAME/Assignment1_Ollama_Chatbot.git
   cd Assignment1_Ollama_Chatbot
   ```

3. **Create a feature branch**
   ```bash
   git checkout -b feature/your-feature-name
   ```

4. **Make your changes**
   - Write clean, documented code
   - Follow existing code style
   - Add comments where needed

5. **Test your changes**
   ```bash
   cd scripts
   ./run_tests.sh
   # Ensure all tests pass
   ```

6. **Commit your changes**
   ```bash
   git add .
   git commit -m "Add feature: your feature description"
   ```

7. **Push to your fork**
   ```bash
   git push origin feature/your-feature-name
   ```

8. **Create Pull Request**
   - Go to GitHub
   - Click "New Pull Request"
   - Describe your changes

### Code Style

- **Python:** Follow PEP 8
- **Comments:** Use clear, descriptive comments
- **Functions:** Include docstrings
- **Variables:** Use descriptive names

### Areas for Contribution

- üêõ Bug fixes
- ‚ú® New features (conversation history, export, themes)
- üìù Documentation improvements
- üß™ Additional tests
- üåê Internationalization
- ‚ö° Performance optimizations

---

## üìÑ License

This project is licensed under the **MIT License**.

```
MIT License

Copyright (c) 2025 Fouad Azem

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## üôè Acknowledgments

- **Ollama** - For providing the local LLM server
- **Streamlit** - For the amazing web framework
- **Flask** - For the lightweight REST API framework
- **Meta AI** - For Llama models
- **Mistral AI** - For Mistral model
- **Microsoft** - For Phi model
- **Meta AI** - For CodeLlama model

---

## üìû Contact & Support

- **GitHub Repository:** https://github.com/fouada/Assignment1_Ollama_Chatbot
- **Issues:** https://github.com/fouada/Assignment1_Ollama_Chatbot/issues
- **Author:** Fouad Azem

---

## üöÄ Quick Start Summary

```bash
# 1. Install dependencies
brew install ollama python3
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. Clone repository
git clone https://github.com/fouada/Assignment1_Ollama_Chatbot.git
cd Assignment1_Ollama_Chatbot

# 3. Start Ollama
brew services start ollama

# 4. Pull model
ollama pull llama3.2

# 5. Setup Python environment
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt

# 6. Run tests
cd scripts
./run_tests.sh

# 7. Launch application
./launch_streamlit.sh  # Web UI
# OR
./launch_flask.sh      # REST API

# 8. Open browser
# Streamlit: http://localhost:8501
# Flask API: http://localhost:5000
```

---

**üéâ Enjoy your private, local AI chatbot!**

**Remember:** All your data stays on your machine. No API keys, no subscriptions, no internet required. Just pure, private AI power at your fingertips.
