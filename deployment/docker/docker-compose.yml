# Docker Compose configuration for Ollama Chatbot
# Production-ready orchestration

version: '3.8'

services:
  ollama-chatbot:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ollama-chatbot
    image: ollama-chatbot:latest
    
    # Port mapping
    ports:
      - "8501:8501"  # Streamlit UI
      - "5000:5000"  # Flask REST API
      - "11434:11434"  # Ollama API
    
    # Volume mounts for persistence
    volumes:
      - ollama-models:/root/.ollama  # Persist downloaded models
      - ./logs:/app/logs  # Persist logs on host
    
    # Environment configuration
    environment:
      - LOG_LEVEL=INFO
      - OLLAMA_HOST=0.0.0.0
      - PYTHONUNBUFFERED=1
      - TZ=UTC
    
    # Resource limits (adjust based on your hardware)
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
    
    # Restart policy
    restart: unless-stopped
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Network configuration
    networks:
      - ollama-network

# Named volumes for data persistence
volumes:
  ollama-models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.ollama

# Network configuration
networks:
  ollama-network:
    driver: bridge

# ============================================
# Usage Instructions
# ============================================
#
# Build and start:
#   docker-compose up -d --build
#
# View logs:
#   docker-compose logs -f
#
# Stop services:
#   docker-compose down
#
# Remove volumes (caution: deletes models):
#   docker-compose down -v
#
# Access services:
#   Streamlit UI:  http://localhost:8501
#   Flask API:     http://localhost:5000
#   Ollama API:    http://localhost:11434
#
# ============================================

