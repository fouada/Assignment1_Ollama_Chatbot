# ðŸ“š Comprehensive Prompt Engineering Guide
## High-Level Communication with LLMs

**Version:** 1.0
**Author:** Research Team
**Purpose:** Demonstrate sophisticated LLM interaction patterns and prompt engineering techniques

---

## Table of Contents

1. [Introduction to Prompt Engineering](#1-introduction)
2. [Fundamental Principles](#2-fundamental-principles)
3. [Prompt Structure Patterns](#3-prompt-structure-patterns)
4. [Research & Analysis Prompts](#4-research--analysis-prompts)
5. [Multi-Turn Conversations](#5-multi-turn-conversations)
6. [Advanced Techniques](#6-advanced-techniques)
7. [Domain-Specific Prompts](#7-domain-specific-prompts)
8. [Best Practices & Anti-Patterns](#8-best-practices)
9. [Prompt Templates Library](#9-prompt-templates)

---

## 1. Introduction to Prompt Engineering

### What is Prompt Engineering?

Prompt engineering is the art and science of crafting inputs to language models to achieve desired outputs. It involves understanding:

- **Model capabilities and limitations**
- **Context window management**
- **Token efficiency**
- **Response formatting**
- **Chain-of-thought reasoning**

### Why It Matters for This Project

Our Ollama Chatbot system demonstrates:
- âœ… Sophisticated prompt patterns for research analysis
- âœ… Multi-turn conversation management
- âœ… Context-aware responses
- âœ… Task decomposition strategies
- âœ… Quality optimization techniques

---

## 2. Fundamental Principles

### Principle 1: Clarity and Specificity

**Poor Prompt:**
```
Tell me about AI
```

**Good Prompt:**
```
Explain the key differences between supervised and unsupervised
machine learning, focusing on:
1. Training data requirements
2. Use cases for each approach
3. Performance evaluation methods

Provide concrete examples for each point.
```

**Why Better:** Specific requirements, structured format, actionable scope.

---

### Principle 2: Context Provision

**Poor Prompt:**
```
How do I optimize this?
```

**Good Prompt:**
```
I'm running a chatbot with the following configuration:
- Model: llama3.2
- Temperature: 1.0
- Average response time: 3.2 seconds
- Quality score: 0.68

Based on research findings, what temperature setting would
optimize the quality/performance tradeoff? Please explain
the reasoning behind your recommendation.
```

**Why Better:** Complete context, specific metrics, clear objective.

---

### Principle 3: Output Format Specification

**Poor Prompt:**
```
List some programming languages
```

**Good Prompt:**
```
List 5 programming languages suitable for machine learning development.
Format your response as:

| Language | Primary Use Case | Key Library |
|----------|------------------|-------------|
| ...      | ...              | ...         |

Include a brief explanation (1-2 sentences) below the table.
```

**Why Better:** Specifies exact format, quantity, and structure.

---

### Principle 4: Role Assignment

**Poor Prompt:**
```
What's the best model to use?
```

**Good Prompt:**
```
You are an expert machine learning engineer evaluating LLM models
for production deployment.

Given these requirements:
- Response time: < 2 seconds (p95)
- Quality threshold: > 0.80
- Resource constraints: 16GB RAM

Analyze the following model comparison data and recommend the
optimal model with justification:

[Model comparison data here]

Provide your recommendation in this format:
1. Recommended model
2. Key advantages
3. Potential risks
4. Mitigation strategies
```

**Why Better:** Clear role, specific criteria, structured output.

---

## 3. Prompt Structure Patterns

### Pattern 1: Task Decomposition

**Use Case:** Complex multi-step problems

```
Task: Analyze chatbot performance and provide optimization recommendations

Please approach this systematically:

Step 1: Data Analysis
- Identify key performance metrics
- Calculate statistical summaries (mean, std, percentiles)
- Detect anomalies or outliers

Step 2: Problem Identification
- Compare current performance to benchmarks
- Identify bottlenecks
- Determine root causes

Step 3: Solution Design
- Propose specific optimizations
- Estimate impact of each change
- Prioritize by ROI

Step 4: Implementation Plan
- Break down into actionable steps
- Identify dependencies
- Set success criteria

Provide detailed analysis for each step.
```

**Why Effective:** Guides LLM through logical progression, ensures completeness.

---

### Pattern 2: Few-Shot Learning

**Use Case:** Establishing response format through examples

```
Convert research findings to executive summary format.

Example 1:
Input: "Temperature sensitivity analysis shows correlation coefficient
of -0.23 between temperature and quality score, with optimal value
at 0.72 based on 50 data points."

Output: "âœ“ Optimal temperature: 0.72 (validated across 50 tests)"

Example 2:
Input: "Statistical comparison using independent t-test shows llama3.2
has significantly lower latency than mistral (p=0.0023, Cohen's d=0.67)"

Output: "âœ“ llama3.2 is 67% faster (statistically significant, p<0.05)"

Now convert this finding:
Input: "Mathematical proof establishes O(n) time complexity for plugin
execution with guaranteed termination in finite time T â‰¤ Î£áµ¢â‚Œâ‚â¿ Táµ¢"

Output: [Your response here]
```

**Why Effective:** Clear pattern recognition, consistent formatting.

---

### Pattern 3: Chain-of-Thought Reasoning

**Use Case:** Complex reasoning tasks

```
Problem: Why does increasing temperature from 0.7 to 1.2 decrease
quality scores while increasing response diversity?

Please reason through this step-by-step:

Step 1: Define what temperature does mathematically
- How does temperature affect the softmax function?
- What happens to probability distribution as T increases?

Step 2: Analyze impact on token selection
- Compare argmax vs sampling at different temperatures
- How does this affect response predictability?

Step 3: Connect to quality metrics
- Which quality dimensions are affected? (coherence, relevance, etc.)
- Why would higher diversity lead to lower scores?

Step 4: Synthesize conclusion
- Summarize the causal chain
- Identify the optimal balance point

Think through each step carefully before providing your final answer.
```

**Why Effective:** Forces structured reasoning, reduces logical errors.

---

### Pattern 4: Constraint Specification

**Use Case:** Bounded solution space

```
Recommend a model configuration for production deployment.

HARD CONSTRAINTS (must satisfy):
- Response time: < 2.0 seconds (p95)
- Memory usage: < 8GB
- Quality score: > 0.75
- Cost: Free/open-source models only

SOFT CONSTRAINTS (prefer but not required):
- Streaming support
- Multi-language capability
- Low variance (CV < 20%)

AVAILABLE OPTIONS:
1. llama3.2: 2.1s latency, 0.85 quality, 6GB RAM
2. mistral: 1.8s latency, 0.82 quality, 7GB RAM
3. phi3: 2.5s latency, 0.87 quality, 5GB RAM

Provide:
1. Your selection (or indicate if none meet constraints)
2. Reasoning for each constraint
3. Tradeoffs analysis
4. Risk assessment
```

**Why Effective:** Clear decision criteria, prevents invalid solutions.

---

## 4. Research & Analysis Prompts

### Prompt 1: Sensitivity Analysis Request

```
Conduct a comprehensive sensitivity analysis on the temperature parameter.

OBJECTIVE:
Determine the optimal temperature setting that maximizes the
quality/performance tradeoff for general-purpose chatbot usage.

METHODOLOGY:
1. Test temperature range: 0.0 to 2.0 in 0.2 increments
2. For each temperature value:
   - Run 5 trials with diverse prompts
   - Measure: response time, quality score, token count
   - Calculate: mean, standard deviation, confidence intervals

3. Statistical Analysis:
   - Compute Pearson correlation (temperature vs quality)
   - Identify inflection points in the relationship
   - Determine optimal value using quality/time ratio

EXPECTED OUTPUT:
1. Correlation coefficient with interpretation
2. Optimal temperature value with confidence interval
3. Performance tradeoff visualization description
4. Actionable recommendation with reasoning

CONSTRAINTS:
- Use consistent prompts across all temperature values
- Control for model version and system load
- Report statistical significance (p-values)
```

---

### Prompt 2: Model Comparison Analysis

```
Perform statistical comparison of LLM models for production selection.

RESEARCH QUESTION:
Which model provides the best overall performance considering
latency, quality, and consistency?

HYPOTHESIS:
Hâ‚€: All models have equivalent performance
Hâ‚: At least one model differs significantly

DATA PROVIDED:
[Insert benchmark data: latency distributions, quality scores,
throughput measurements for each model]

ANALYSIS REQUIRED:

1. Descriptive Statistics:
   - Mean, median, std dev for each metric
   - Distribution shape (skewness, kurtosis)
   - Outlier detection and handling

2. Inferential Statistics:
   - Independent samples t-tests (pairwise comparisons)
   - Effect size calculation (Cohen's d)
   - Confidence intervals (95%)
   - Determine statistical significance (Î± = 0.05)

3. Practical Significance:
   - Composite scoring (weighted metrics)
   - Rank models by overall performance
   - Identify when differences matter in practice

4. Recommendation:
   - Best overall model with justification
   - Task-specific recommendations (if applicable)
   - Confidence level in recommendation

FORMAT: Present findings as formal research summary with
statistical rigor.
```

---

### Prompt 3: Mathematical Proof Verification

```
Verify the following theorem and provide formal proof.

THEOREM: Plugin System Completeness
"A plugin system with n plugins, where each plugin páµ¢ has finite
execution time Táµ¢ and dependencies form a DAG, always terminates
in finite time T â‰¤ Î£áµ¢â‚Œâ‚â¿ Táµ¢"

PROOF REQUIREMENTS:

1. State all assumptions explicitly
   - What properties must the system have?
   - What are the boundary conditions?

2. Choose proof technique
   - Mathematical induction
   - Direct proof
   - Proof by contradiction
   - Other (specify)

3. Provide rigorous proof
   - Base case (if induction)
   - Inductive step (if induction)
   - Logical derivation with clear steps
   - Cite relevant theorems/lemmas

4. Complexity analysis
   - Time complexity: Big-O notation
   - Space complexity: Memory requirements
   - Best/average/worst case scenarios

5. Practical implications
   - What does this guarantee for real systems?
   - Are there performance bottlenecks?
   - How does this scale?

NOTATION: Use standard mathematical notation (âˆ€, âˆƒ, âŸ¹, etc.)
RIGOR: Each step must follow logically from previous steps
```

---

### Prompt 4: Quality Assessment Framework

```
Design a comprehensive quality assessment framework for LLM responses.

OBJECTIVE:
Create a multi-dimensional scoring system that evaluates:
1. Factual accuracy
2. Coherence and logical flow
3. Relevance to prompt
4. Completeness of answer
5. Clarity of expression

REQUIREMENTS:

Part 1: Define Quality Dimensions
For each dimension:
- Precise definition
- Measurable criteria (quantitative where possible)
- Examples of high/medium/low quality
- Weighting justification

Part 2: Scoring Methodology
- Scale definition (e.g., 0-1, 1-5, etc.)
- Calculation method for composite score
- Handling edge cases
- Normalization approach

Part 3: Validation Strategy
- How to validate scoring accuracy?
- Inter-rater reliability measures
- Calibration dataset requirements
- Continuous improvement process

Part 4: Implementation
- Provide pseudocode for scoring algorithm
- Identify automation opportunities
- Specify manual review points
- Error handling procedures

DELIVERABLE: Complete framework document ready for implementation
```

---

## 5. Multi-Turn Conversations

### Pattern: Iterative Refinement

**Turn 1: Initial Request**
```
USER: Analyze the performance data and identify optimization opportunities.

[Data provided]
```

**Turn 2: Clarification**
```
ASSISTANT: I've identified three potential optimizations:
1. Temperature adjustment (expected +15% quality)
2. Model switching (expected -20% latency)
3. Streaming mode (expected +40% perceived speed)

Which optimization is your priority, or should I analyze the
combined impact of all three?

USER: Analyze the combined impact, but prioritize quality improvements.
```

**Turn 3: Deep Dive**
```
ASSISTANT: [Detailed analysis]

USER: The temperature adjustment looks promising. What specific value
do you recommend and why?

ASSISTANT: Based on sensitivity analysis:
- Current temperature: 1.0
- Recommended: 0.72
- Expected improvement: +12% quality, +3% speed

Reasoning:
1. Correlation analysis shows optimal at 0.72
2. Quality/performance ratio peaks at this value
3. Statistical validation: 50 trials, p<0.05

Would you like me to:
A) Provide implementation steps?
B) Analyze potential risks?
C) Suggest monitoring strategy?
```

**Why Effective:** Progressive refinement, maintains context, actionable options.

---

### Pattern: Socratic Dialogue

**Turn 1: Open-ended Question**
```
USER: Our chatbot response time is too slow. What should we do?
```

**Turn 2: Diagnostic Questions**
```
ASSISTANT: Let's diagnose this systematically. I need some information:

1. What is "too slow" specifically?
   - Current p95 latency?
   - User expectation?
   - Business requirement?

2. When does slowness occur?
   - All queries or specific types?
   - Peak hours or always?
   - Certain models only?

3. What have you tried already?
   - Any configuration changes?
   - Performance monitoring in place?
   - Root cause analysis done?

USER: p95 is 3.2 seconds, users expect <2 seconds. Happens for
all queries. Haven't tried anything yet.
```

**Turn 3: Hypothesis Formation**
```
ASSISTANT: Based on your input, possible causes:
1. Temperature too high (sampling overhead)
2. Non-optimal model selection
3. Lack of streaming (perceived latency)
4. System resource constraints

Priority hypothesis: Temperature and streaming combination.

Here's why: Our research shows:
- Streaming reduces perceived latency by 40-50%
- Temperature >0.8 adds sampling overhead
- Combined impact could get you to <2s

Let's test this hypothesis. Do you know your current:
- Temperature setting?
- Streaming status (enabled/disabled)?

USER: Temperature is 1.2, streaming is disabled.
```

**Turn 4: Solution Proposal**
```
ASSISTANT: Perfect! High-confidence solution:

RECOMMENDATION:
1. Reduce temperature: 1.2 â†’ 0.7 (expected: -15% latency)
2. Enable streaming (expected: -45% perceived latency)

EXPECTED OUTCOME:
- Current p95: 3.2s
- After optimization: ~1.8s (within your <2s target)
- Quality impact: Likely improves (temperature 1.2 is suboptimal)

IMPLEMENTATION:
```python
# Configuration change
config = {
    "temperature": 0.7,  # Down from 1.2
    "stream": True       # Enable streaming
}
```

VALIDATION:
Run benchmark after change, measure:
- p95 latency (target: <2.0s)
- Quality score (should maintain or improve)
- User satisfaction metrics

Would you like me to:
A) Provide detailed implementation steps?
B) Suggest monitoring dashboard?
C) Discuss rollback strategy?
```

**Why Effective:** Builds understanding, involves user, high-quality solution.

---

## 6. Advanced Techniques

### Technique 1: Prompt Chaining

**Use Case:** Complex multi-stage analysis

```
STAGE 1 PROMPT:
"Analyze this performance data and extract key statistics:
[Data]

Output ONLY a JSON with: mean, std, min, max, p95, p99"

STAGE 2 PROMPT (uses Stage 1 output):
"Given these statistics:
{output_from_stage_1}

And these benchmarks:
- Acceptable: p95 < 2.0s
- Good: p95 < 1.5s
- Excellent: p95 < 1.0s

Classify the performance and explain why."

STAGE 3 PROMPT (uses Stage 1 & 2):
"Based on:
- Statistics: {stage_1_output}
- Classification: {stage_2_output}

Generate 3 specific optimization recommendations with:
1. Expected impact (quantified)
2. Implementation complexity (1-5)
3. Risk level (low/medium/high)"
```

**Why Effective:** Each stage focuses on one task, outputs compose.

---

### Technique 2: Self-Consistency

**Use Case:** Verify reasoning quality

```
Problem: Determine optimal temperature for production deployment.

INSTRUCTION: Solve this problem THREE times using DIFFERENT
reasoning approaches. Then compare your answers.

Approach 1: Data-driven analysis
[Analyze from experimental data perspective]

Approach 2: Theoretical reasoning
[Analyze from mathematical/theoretical perspective]

Approach 3: Practical constraints
[Analyze from production requirements perspective]

FINAL STEP: Compare all three answers
- If they agree: High confidence in recommendation
- If they disagree: Identify source of disagreement and resolve

Provide your final recommendation with confidence level.
```

**Why Effective:** Multiple reasoning paths increase reliability.

---

### Technique 3: Constrained Generation

**Use Case:** Structured output requirements

```
Generate a research findings summary with EXACTLY this structure:

## Executive Summary
[Exactly 2-3 sentences, <280 characters total]

## Key Findings
[Exactly 3 findings, each one sentence]

## Statistical Evidence
Finding 1: [metric] = [value] (p=[p-value], CI=[range])
Finding 2: [metric] = [value] (p=[p-value], CI=[range])
Finding 3: [metric] = [value] (p=[p-value], CI=[range])

## Recommendations
1. [Action] â†’ [Expected impact]
2. [Action] â†’ [Expected impact]
3. [Action] â†’ [Expected impact]

## Risk Assessment
[Exactly 1 paragraph, 3-4 sentences]

CONSTRAINTS:
- Total length: 200-250 words
- Use bullet points only where specified
- Include specific numbers (no vague terms like "significant")
- Every claim must have supporting data reference
```

---

### Technique 4: Meta-Prompting

**Use Case:** Optimize prompts themselves

```
I need to create a prompt for [task description].

Please help me design an optimal prompt by:

1. Analyzing the task requirements
   - What is the core objective?
   - What are success criteria?
   - What could go wrong?

2. Identifying key components needed
   - Context to provide
   - Constraints to specify
   - Output format requirements

3. Suggesting prompt structure
   - Opening (role/context)
   - Body (specific instructions)
   - Closing (format/validation)

4. Providing the final prompt
   - Complete, ready-to-use prompt
   - Explanation of key design choices
   - Expected output quality

EXAMPLE TASK: "Analyze chatbot latency and recommend optimizations"

Design the optimal prompt for this task.
```

---

## 7. Domain-Specific Prompts

### Domain 1: Performance Optimization

```
PROMPT: Chatbot Performance Diagnostics

SYSTEM CONTEXT:
- Model: {model_name}
- Temperature: {temp_value}
- Streaming: {enabled/disabled}
- Average latency: {latency}
- Quality score: {quality}

PROBLEM:
Response times exceed user expectations.

DIAGNOSTIC PROTOCOL:

Phase 1: Metric Analysis
â–¡ Is latency consistent or variable? (check CV%)
â–¡ Are there outliers? (check p99 vs p95)
â–¡ Does quality meet threshold? (>0.75)
â–¡ Is streaming available and beneficial?

Phase 2: Bottleneck Identification
â–¡ Model selection appropriate for use case?
â–¡ Temperature optimal for quality/speed tradeoff?
â–¡ Resource constraints (RAM, CPU)?
â–¡ Network latency (if applicable)?

Phase 3: Optimization Opportunities
Rank by ROI:
1. [Optimization] - Impact: [X%] - Effort: [Low/Med/High]
2. [Optimization] - Impact: [X%] - Effort: [Low/Med/High]
3. [Optimization] - Impact: [X%] - Effort: [Low/Med/High]

Phase 4: Implementation Plan
For top recommendation:
- Step-by-step instructions
- Expected impact (quantified)
- Validation approach
- Rollback strategy

DELIVERABLE: Complete optimization plan with confidence assessment
```

---

### Domain 2: Quality Assessment

```
PROMPT: Multi-Dimensional Quality Evaluation

TASK: Evaluate this chatbot response across multiple dimensions.

RESPONSE TO EVALUATE:
"{response_text}"

ORIGINAL PROMPT:
"{user_prompt}"

EVALUATION FRAMEWORK:

1. Factual Accuracy (0-1 score)
   â–¡ Are factual claims correct?
   â–¡ Are there any hallucinations?
   â–¡ Score: ___ / 1.0

2. Relevance (0-1 score)
   â–¡ Does it address the prompt directly?
   â–¡ Is information on-topic?
   â–¡ Score: ___ / 1.0

3. Coherence (0-1 score)
   â–¡ Is the logic flow clear?
   â–¡ Are sentences well-structured?
   â–¡ Score: ___ / 1.0

4. Completeness (0-1 score)
   â–¡ Are all aspects of the prompt covered?
   â–¡ Is the answer thorough?
   â–¡ Score: ___ / 1.0

5. Clarity (0-1 score)
   â–¡ Is language clear and concise?
   â–¡ Are complex ideas well-explained?
   â–¡ Score: ___ / 1.0

COMPOSITE SCORE:
(Weighted average: Relevance 30%, Accuracy 25%, Coherence 20%,
 Completeness 15%, Clarity 10%)

Total: ___ / 1.0

IMPROVEMENT SUGGESTIONS:
1. [Specific issue] â†’ [How to fix]
2. [Specific issue] â†’ [How to fix]
3. [Specific issue] â†’ [How to fix]

REVISED RESPONSE (if score < 0.8):
[Provide improved version addressing identified issues]
```

---

### Domain 3: Research Analysis

```
PROMPT: Comprehensive Research Data Analysis

DATA CONTEXT:
- Experiment type: {sensitivity_analysis/comparison/other}
- Sample size: n = {number}
- Variables: {list variables}
- Hypothesis: {research hypothesis}

RAW DATA:
{json_data_or_table}

ANALYSIS PROTOCOL:

STEP 1: Data Quality Check
â–¡ Missing values? (report %)
â–¡ Outliers? (identify using IQR method)
â–¡ Normality? (assess distribution shape)
â–¡ Sample size adequate? (power analysis)

STEP 2: Descriptive Statistics
- Central tendency: mean, median, mode
- Dispersion: std, variance, range
- Shape: skewness, kurtosis
- Visualization: describe appropriate chart type

STEP 3: Inferential Statistics
- Choose test: {t-test/ANOVA/regression/other}
- Assumptions: {verify test assumptions}
- Execute test: calculate test statistic
- Results: p-value, confidence intervals

STEP 4: Effect Size
- Calculate: Cohen's d / Î·Â² / r
- Interpret: negligible/small/medium/large
- Practical significance assessment

STEP 5: Conclusions
- Accept or reject null hypothesis (justify)
- Confidence in findings (low/medium/high)
- Limitations of analysis
- Recommendations for action

DELIVERABLE FORMAT:
## Statistical Analysis Report

### Summary
[2-3 sentences with key finding]

### Methods
[Statistical procedures used]

### Results
[Numerical results with interpretation]

### Conclusions
[Actionable insights with confidence level]
```

---

## 8. Best Practices & Anti-Patterns

### âœ… Best Practices

#### 1. Be Specific and Concrete
```
âŒ BAD: "Make it better"
âœ… GOOD: "Reduce response time by 20% while maintaining quality score above 0.80"
```

#### 2. Provide Examples
```
âŒ BAD: "Format as table"
âœ… GOOD: "Format as markdown table:
| Metric | Value |
|--------|-------|
| Speed  | 2.3s  |"
```

#### 3. Set Expectations
```
âŒ BAD: "Analyze this data"
âœ… GOOD: "Analyze this data and provide:
1. Mean and standard deviation
2. Identification of outliers
3. Top 3 insights
Format: bullet points, ~200 words"
```

#### 4. Use Structured Thinking
```
âŒ BAD: "What should we do?"
âœ… GOOD: "Analyze this problem step-by-step:
1. Identify root causes
2. Generate 3 solutions
3. Rank by feasibility
4. Recommend top choice with reasoning"
```

#### 5. Iterate and Refine
```
Initial: "Optimize configuration"
Refined: "Optimize temperature parameter based on quality/speed tradeoff"
Final: "Determine optimal temperature value (0.0-2.0 range) that
maximizes quality score while keeping p95 latency under 2.0 seconds"
```

---

### âŒ Anti-Patterns to Avoid

#### 1. Vague Objectives
```
âŒ "Improve the system"
âŒ "Make it faster"
âŒ "Better quality"

Why bad: No measurable target, unclear success criteria
```

#### 2. Missing Context
```
âŒ "Is this good?" [shows number without context]
âŒ "What should I change?" [no current state provided]

Why bad: LLM cannot provide relevant guidance
```

#### 3. Overloading Single Prompt
```
âŒ "Analyze data, create visualizations, write report, suggest
optimizations, implement changes, and deploy to production"

Why bad: Too many tasks, likely poor quality for each

âœ… Better: Break into separate prompts or use prompt chaining
```

#### 4. Assuming Knowledge
```
âŒ "Use the standard method"
âŒ "Apply normal configuration"
âŒ "Fix the obvious issue"

Why bad: LLM doesn't know your specific context

âœ… Better: Explicitly state methods, values, and issues
```

#### 5. No Validation Criteria
```
âŒ "Give me a recommendation"

Why bad: No way to evaluate if recommendation is appropriate

âœ… Better: "Recommend a configuration that meets:
- Latency < 2s
- Quality > 0.80
- Memory < 8GB
Explain how your recommendation satisfies each criterion"
```

---

## 9. Prompt Templates Library

### Template 1: Research Question Investigation

```
RESEARCH QUESTION: {your_question}

BACKGROUND:
{relevant_context_and_prior_knowledge}

METHODOLOGY:
Please investigate this question systematically:

1. Literature/Data Review
   - What is known about this topic?
   - What are the key variables?
   - What have others found?

2. Hypothesis Formation
   - What is your hypothesis?
   - What evidence supports it?
   - What would disprove it?

3. Analysis Approach
   - What data/methods are needed?
   - How to control for confounds?
   - What are the limitations?

4. Findings
   - Present results clearly
   - Statistical evidence
   - Confidence level

5. Implications
   - What does this mean?
   - Actionable insights
   - Future directions

FORMAT: Formal research summary with clear structure
```

---

### Template 2: Comparative Analysis

```
COMPARISON TASK: {what_to_compare}

OPTIONS:
Option A: {description_a}
Option B: {description_b}
Option C: {description_c}

EVALUATION CRITERIA:
1. {criterion_1} (weight: {w1}%)
2. {criterion_2} (weight: {w2}%)
3. {criterion_3} (weight: {w3}%)

CONSTRAINTS:
- Must satisfy: {hard_constraints}
- Prefer: {soft_constraints}

ANALYSIS REQUIRED:

Part 1: Individual Assessment
For each option, rate on each criterion (1-10 scale):
[Table format with scores and justifications]

Part 2: Weighted Scoring
Calculate composite scores using weights above

Part 3: Tradeoff Analysis
- Where does each option excel?
- Where does each option fall short?
- Are there deal-breakers?

Part 4: Recommendation
- Top choice with reasoning
- Alternative if top choice unavailable
- Confidence level (low/med/high)

Part 5: Sensitivity Analysis
- How sensitive is recommendation to weight changes?
- What would change the outcome?
```

---

### Template 3: Problem-Solving Framework

```
PROBLEM: {clear_problem_statement}

CURRENT STATE:
- Metric 1: {value}
- Metric 2: {value}
- Context: {relevant_information}

DESIRED STATE:
- Metric 1: {target_value}
- Metric 2: {target_value}
- Timeline: {deadline}

CONSTRAINTS:
- Budget: {constraint}
- Resources: {constraint}
- Requirements: {constraint}

SOLVING APPROACH:

Stage 1: Problem Decomposition
Break the problem into sub-problems:
1. {sub-problem_1}
2. {sub-problem_2}
3. {sub-problem_3}

Stage 2: Root Cause Analysis
For each sub-problem, identify:
- Immediate causes
- Contributing factors
- Underlying root causes

Stage 3: Solution Generation
For each root cause:
- Propose 2-3 solutions
- Estimate impact (low/med/high)
- Assess feasibility (easy/moderate/hard)

Stage 4: Solution Selection
Rank solutions by ROI:
1. {solution} - Impact: {X} - Effort: {Y}
2. {solution} - Impact: {X} - Effort: {Y}
3. {solution} - Impact: {X} - Effort: {Y}

Stage 5: Implementation Plan
For top solution:
- Detailed steps (numbered list)
- Timeline and dependencies
- Success metrics
- Risk mitigation

DELIVERABLE: Actionable plan ready for execution
```

---

### Template 4: Data Interpretation

```
DATA INTERPRETATION TASK

DATASET CONTEXT:
- Source: {where_data_comes_from}
- Size: {n_observations}
- Variables: {list_variables}
- Collection method: {how_collected}

DATA:
{provide_data_in_structured_format}

INTERPRETATION FRAMEWORK:

Level 1: What (Descriptive)
- What patterns are visible?
- What are the key statistics?
- What stands out?

Level 2: So What (Analysis)
- Why do these patterns exist?
- What relationships are present?
- What is surprising/expected?

Level 3: Now What (Actionable)
- What decisions should this inform?
- What actions should be taken?
- What further investigation is needed?

SPECIFIC QUESTIONS:
1. {question_1}
2. {question_2}
3. {question_3}

DELIVERABLE FORMAT:
## Data Interpretation Report

### Key Observations
[Bullet points with specific numbers]

### Analysis
[Explanation of patterns and relationships]

### Insights
[3-5 actionable insights with confidence levels]

### Recommendations
[Specific actions based on data]

### Caveats
[Limitations and uncertainties]
```

---

### Template 5: Code/System Review

```
REVIEW REQUEST: {what_to_review}

CODE/SYSTEM DESCRIPTION:
{description_of_system_or_code}

REVIEW CRITERIA:

1. Correctness
   â–¡ Does it work as intended?
   â–¡ Are there edge cases not handled?
   â–¡ Are there logical errors?

2. Performance
   â–¡ Time complexity: {analyze}
   â–¡ Space complexity: {analyze}
   â–¡ Bottlenecks: {identify}

3. Reliability
   â–¡ Error handling adequate?
   â–¡ Recovery mechanisms present?
   â–¡ Testing coverage?

4. Maintainability
   â–¡ Code clarity and documentation
   â–¡ Modularity and organization
   â–¡ Technical debt assessment

5. Security
   â–¡ Input validation
   â–¡ Authentication/authorization
   â–¡ Vulnerability assessment

REVIEW OUTPUT:

### Summary
[Overall assessment: Good/Needs Work/Critical Issues]

### Strengths
1. {strength_1}
2. {strength_2}
3. {strength_3}

### Issues
Priority 1 (Critical):
- {issue} â†’ {impact} â†’ {fix}

Priority 2 (Important):
- {issue} â†’ {impact} â†’ {fix}

Priority 3 (Nice-to-have):
- {issue} â†’ {impact} â†’ {fix}

### Recommendations
[Prioritized list of improvements]

### Approval Status
â–¡ Approved as-is
â–¡ Approved with minor changes
â–¡ Requires revision
â–¡ Requires major rework
```

---

## 10. Advanced Use Cases

### Use Case 1: Automated Research Pipeline

```
OBJECTIVE: Automate entire research workflow

PIPELINE STAGES:

Stage 1: Experiment Design
Prompt: "Design experiment to test hypothesis: {hypothesis}
Requirements: {requirements}
Output: Experiment protocol in JSON format"

Stage 2: Data Collection
[Automated experiment execution]
Output: Raw data in structured format

Stage 3: Statistical Analysis
Prompt: "Analyze this data using {test_type}:
{data}
Provide: test statistic, p-value, effect size, interpretation"

Stage 4: Visualization
Prompt: "Describe optimal visualization for:
- Data type: {type}
- Key message: {message}
- Audience: {audience}
Output: Chart type, axes, annotations"

Stage 5: Report Generation
Prompt: "Generate research report from:
- Hypothesis: {hypothesis}
- Methods: {methods}
- Results: {results}
Format: Executive summary + detailed findings"

Stage 6: Recommendation
Prompt: "Based on findings:
{findings}
Provide: actionable recommendations ranked by ROI"

INTEGRATION: Each stage output feeds into next stage
```

---

### Use Case 2: Interactive Debugging

```
DEBUGGING SESSION: {system_name}

SYMPTOM: {observed_behavior}
EXPECTED: {correct_behavior}

DEBUG PROTOCOL:

Round 1: Information Gathering
Assistant: "I need these diagnostics:
1. {diagnostic_1}
2. {diagnostic_2}
3. {diagnostic_3}
Please provide values/logs"

User: [provides information]

Round 2: Hypothesis Formation
Assistant: "Based on diagnostics, possible causes:
A) {cause_a} (probability: high/med/low)
B) {cause_b} (probability: high/med/low)
C) {cause_c} (probability: high/med/low)

To test hypothesis A, please:
{test_instructions}

What are the results?"

User: [provides test results]

Round 3: Diagnosis
Assistant: "Test results indicate {cause}.
Root cause: {detailed_explanation}

Fix: {solution}
Verification: {how_to_verify}
Prevention: {how_to_prevent_recurrence}"

DELIVERABLE: Complete debugging report with fix
```

---

### Use Case 3: Optimization Loop

```
OPTIMIZATION OBJECTIVE: {goal}

CURRENT BASELINE:
- Metric 1: {value}
- Metric 2: {value}

TARGET:
- Metric 1: {target}
- Metric 2: {target}

ITERATIVE OPTIMIZATION:

Iteration 1:
Prompt: "Current state: {baseline}
        Target: {target}
        Suggest one optimization to move closer to target.
        Provide: specific change, expected impact, implementation"

[Implement suggested change]
[Measure new metrics]

Iteration 2:
Prompt: "Previous optimization: {change}
        Impact: {actual_impact} (expected: {expected_impact})
        New state: {new_metrics}
        Target: {target}

        Analysis:
        - Did it work as expected?
        - What to optimize next?
        - Suggest next iteration"

[Continue until target reached or diminishing returns]

Final Report:
Prompt: "Optimization complete.
        Starting state: {baseline}
        Final state: {final}
        Changes made: {list_all_changes}

        Generate:
        1. Summary of improvements
        2. ROI analysis
        3. Lessons learned
        4. Maintenance recommendations"
```

---

## 11. Evaluation and Quality Assurance

### Prompt Quality Checklist

Before sending a prompt, verify:

- [ ] **Clear objective**: Can someone else understand what you want?
- [ ] **Sufficient context**: Is all necessary information provided?
- [ ] **Specific constraints**: Are boundaries and requirements explicit?
- [ ] **Output format**: Is desired format clearly specified?
- [ ] **Success criteria**: How will you know if the response is good?
- [ ] **Edge cases**: Have you considered potential issues?
- [ ] **Token efficiency**: Is the prompt concise yet complete?

---

### Response Quality Assessment

Evaluate LLM responses on:

1. **Relevance** (0-10): Does it address the prompt?
2. **Accuracy** (0-10): Are facts correct?
3. **Completeness** (0-10): Are all aspects covered?
4. **Clarity** (0-10): Is it easy to understand?
5. **Actionability** (0-10): Can you act on it?

**Composite Score** = Average of above
- 9-10: Excellent
- 7-8: Good
- 5-6: Acceptable
- 0-4: Needs improvement

---

## 12. Conclusion

### Key Takeaways

1. **Specificity matters**: Clear, detailed prompts get better results
2. **Structure helps**: Organized prompts lead to organized responses
3. **Context is critical**: Provide all necessary background
4. **Iteration improves**: Refine prompts based on results
5. **Templates accelerate**: Reusable patterns save time

### Continuous Improvement

- **Collect examples**: Save prompts that work well
- **Analyze failures**: Learn from poor responses
- **Share patterns**: Build team prompt library
- **Measure results**: Track prompt effectiveness
- **Stay updated**: LLM capabilities evolve

---

## 13. Quick Reference

### Prompt Engineering Checklist

```
â–¡ Clear objective stated
â–¡ Sufficient context provided
â–¡ Specific constraints included
â–¡ Output format specified
â–¡ Examples given (if needed)
â–¡ Success criteria defined
â–¡ Edge cases considered
â–¡ Token budget appropriate
â–¡ Multi-turn strategy planned
â–¡ Validation method ready
```

### Common Prompt Patterns

1. **Task + Context + Format**
2. **Role + Problem + Solution Request**
3. **Data + Analysis + Insights**
4. **Compare + Criteria + Recommendation**
5. **Current State + Target + Plan**

### Emergency Prompt Fixes

If response is poor:
1. Add more context
2. Be more specific
3. Provide examples
4. Break into smaller tasks
5. Specify output format
6. Add constraints
7. Request step-by-step reasoning

---

**End of Prompt Engineering Guide**

*For project-specific usage, see RESEARCH_QUICKSTART.md and DASHBOARD_GUIDE.md*
